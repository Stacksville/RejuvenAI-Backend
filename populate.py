# prepopulates vector embeddings into the vectordb
from pathlib import Path
from langchain_text_splitters import MarkdownTextSplitter,RecursiveCharacterTextSplitter
from langchain.indexes import IndexingResult, SQLRecordManager, index
from langchain_community.document_loaders import HuggingFaceDatasetLoader
from langchain_core.documents import Document
from langchain_core.vectorstores import VectorStore
from langchain_docling import DoclingLoader
from docling.chunking import HybridChunker
from langchain_docling.loader import ExportType

from vectordb import get_vectordb

EXPORT_TYPE = ExportType.DOC_CHUNKS

def restructure_docling_metadata(mt: dict) -> dict: 
    """
    Helper to reformat the metadata generated by docling 
    to one that is understood by chromadb
    """


    metadata = {
        "source": mt["source"],
        "page": mt["dl_meta"]["doc_items"][0]["prov"][0]["page_no"],
        "format": mt["dl_meta"]["origin"]["mimetype"],
    }

    return metadata


def process_pdf_docs(datasource_path: str) -> list[Document]: 
    """
        processes locally stored pdf documents into a list of Document objects
    """

    dir = Path(datasource_path)
    files = [str(file) for file in dir.glob("*.pdf")]

    docs = []  ##type: List[Document]
    docs_treated = []

    loader = DoclingLoader(file_path=files,export_type = EXPORT_TYPE, chunker=HybridChunker(),)
    
    docs = loader.load()

    splitter = MarkdownTextSplitter()

    docs += splitter.split_documents(docs)

    #restructure metadata produced by Docling
    for doc in docs: 
        doc.metadata = restructure_docling_metadata(doc.metadata)
        docs_treated.append(doc)

    if not docs: 
        print("No mock documents found in the specified directory. Exiting...")
        raise SystemExit

    return docs_treated


def process_hf_ds(repo: str) -> list[Document]: 
    #dataset = load_dataset(repo, "text-corpus", split="test")
    docs =[]
    
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)

    loader = HuggingFaceDatasetLoader(path=repo, page_content_column="passage",name="text-corpus")

    limit = 50

    documents = loader.load()
    docs += text_splitter.split_documents(documents[:limit])

    

    if not docs: 
        print("No mock documents found in the specified directory. Exiting...")
        raise SystemExit

    return docs

def index_documents(docs: list[Document], vectordb: VectorStore, source_field: str) -> IndexingResult: 
    """
        Indexes documents into a vectordb
    """

    namespace = "chromadb/rejuvenai"
    record_manager = SQLRecordManager(
        namespace, db_url="sqlite:///record_manager_cache.sql"
    )
    record_manager.create_schema()

    index_result = index(
        docs,
        record_manager,
        vectordb,
        cleanup="incremental",
        source_id_key=source_field,
    )

    return index_result

def is_context_loaded(): 
    try: 
        vdb = get_vectordb()
        documents = vdb.get(limit=5)["documents"]

    except KeyError: 
        return False 

    if len(documents) > 1: 
        return True

    return False
    

def load_knowledge_base(dataset: str): 

    if is_context_loaded(): 
        print("A vectordb was found. Skipping indexing process")
        return

    dataset_selector = {
        "MOCK": {"source": "./data/mock_data/",  "func": process_pdf_docs,"source_field": "source"},
        "BIOASQ": {"source": "enelpol/rag-mini-bioasq", "func": process_hf_ds, "source_field": "id"},
        "PROD": {"source": "./data/prod_data/", "func": process_pdf_docs, "source_field": "source"}
    }

    ds_conf = dataset_selector.get(dataset)
    if not ds_conf: 
        raise KeyError("Invalid DATASET value")

    vdb = get_vectordb()

    processor_func = ds_conf["func"]
    docs = processor_func(ds_conf["source"])

    source_field = ds_conf["source_field"]
    
    idx_results = index_documents(docs,vdb, source_field)
    print(f"Indexing Result: {idx_results}")

    
